"""
SIMICE service implementation for central site.
Multiple Imputation using Chained Equations for federated learning.
"""

import asyncio
import json
import numpy as np
import pandas as pd
from typing import Dict, Any, List, Set, Optional

from common.algorithm.protocol import create_message, parse_message, MessageType
from .base_algorithm_service import BaseAlgorithmService
from .. import models
from ..websockets.connection_manager import ConnectionManager

# Import the SIMICE algorithm
from algorithms.SIMICE.simice_central import SIMICECentralAlgorithm


class SIMICEService(BaseAlgorithmService):
    """
    Service for the SIMICE algorithm on the central site.
    Multiple Imputation using Chained Equations.
    """
    
    def __init__(self, manager: ConnectionManager):
        """
        Initialize SIMICE service.
        
        Args:
            manager: WebSocket connection manager
        """
        super().__init__(manager)
        self.algorithm = SIMICECentralAlgorithm()
        self.job_data = {}  # Store job-specific data
        self.site_to_job = {}  # Map site IDs to job IDs
    
    async def start_job(self, db_job: models.Job, central_data: pd.DataFrame) -> None:
        """
        Start a new SIMICE job.
        
        Args:
            db_job: Job database record
            central_data: Data from the central site
        """
        job_id = db_job.id
        
        # Create job status for tracking
        job_status = self.job_status_tracker.start_job(job_id)
        job_status.add_message(f"Starting {db_job.algorithm} job with {len(db_job.participants or [])} participants")
        
        # Parse SIMICE parameters
        params = json.loads(db_job.parameters) if isinstance(db_job.parameters, str) else db_job.parameters
        target_column_indexes = params.get('target_column_indexes', [])
        is_binary = params.get('is_binary', [])
        iteration_before_first_imputation = params.get('iteration_before_first_imputation', 5)
        iteration_between_imputations = params.get('iteration_between_imputations', 3)
        imputation_trials = getattr(db_job, 'imputation_trials', 10)  # Get from job model
        
        # Validate parameters
        if not target_column_indexes or not is_binary:
            job_status.fail("Missing required SIMICE parameters: target_column_indexes and is_binary")
            return
        
        if len(target_column_indexes) != len(is_binary):
            job_status.fail("Length mismatch between target_column_indexes and is_binary")
            return
        
        job_status.add_message(f"Target columns: {target_column_indexes}, Binary flags: {is_binary}")
        job_status.add_message(f"Iterations before first: {iteration_before_first_imputation}, between: {iteration_between_imputations}")
        job_status.add_message(f"Imputation trials: {imputation_trials}")
        
        # Store job data
        self.job_data[job_id] = {
            'central_data': central_data,
            'target_column_indexes': target_column_indexes,
            'is_binary': is_binary,
            'iteration_before_first_imputation': iteration_before_first_imputation,
            'iteration_between_imputations': iteration_between_imputations,
            'imputation_trials': imputation_trials,
            'participants': set(db_job.participants or []),
            'connected_sites': set(),
            'site_data': {}
        }
        
        # Notify participants to start
        participants_message = create_message(
            "start_job",  # Use string instead of MessageType.START_JOB
            job_id=job_id,
            algorithm='SIMICE',
            target_column_indexes=target_column_indexes,
            is_binary=is_binary,
            iteration_before_first_imputation=iteration_before_first_imputation,
            iteration_between_imputations=iteration_between_imputations
        )
        
        # Send to all participants
        for participant_id in db_job.participants or []:
            await self.manager.send_to_site(participant_id, participants_message)
        
        job_status.add_message("Notified all participants to start SIMICE job")
        
        # Wait for all participants to connect and send data
        await self._wait_for_participants(job_id)
    
    async def handle_site_message(self, site_id: str, message: str) -> None:
        """
        Handle a message from a remote site.
        
        Args:
            site_id: ID of the site that sent the message
            message: The message content
        """
        print(f"ðŸ”„ SIMICE: Received message from site {site_id}: {message}")
        
        try:
            # Parse message manually like SIMI does
            data = json.loads(message)
            message_type = data.get("type")
            job_id = data.get("job_id")
            
            print(f"ðŸ“¨ SIMICE: Parsed message - type: {message_type}, job_id: {job_id}")
            
            # Handle connect messages first (they always have job_id)
            if message_type == "connect":
                print(f"ðŸ”— SIMICE: Handling connect message from site {site_id}")
                await self._handle_connect(site_id, data)
                return
            
            # For other messages, resolve job_id via mapping if absent
            if job_id is None:
                job_id = self.site_to_job.get(site_id)
                print(f"ðŸ” SIMICE: Resolved job_id for site {site_id}: {job_id}")
                if job_id is None:
                    print(f"âŒ SIMICE: No job mapping found for site {site_id}")
                    return
                # Add job_id to data for convenience
                data["job_id"] = job_id
            
            print(f"ðŸŽ¯ SIMICE: Processing {message_type} for job {job_id} from site {site_id}")
            
            if message_type == "site_ready":
                await self._handle_site_ready(site_id, data)
            elif message_type == "data_ready":
                await self._handle_data_ready(site_id, data)
            elif message_type == "data_summary":
                await self._handle_data_summary(site_id, data)
            elif message_type == "statistics":
                await self._handle_statistics(site_id, data)
            elif message_type == "imputation_updated":
                await self._handle_imputation_updated(site_id, data)
            elif message_type == "statistics_ready":
                await self._handle_statistics_ready(site_id, data)
            elif message_type == "final_data":
                await self._handle_final_data(site_id, data)
            else:
                print(f"âš ï¸ SIMICE: Unknown message type from site {site_id}: {message_type}")
                
        except Exception as e:
            print(f"ðŸ’¥ SIMICE: Error handling message from site {site_id}: {e}")
            import traceback
            traceback.print_exc()
            
            # Update job status with error
            for job_id in self.job_data:
                if site_id in self.job_data[job_id]['participants']:
                    job_status = self.job_status_tracker.get_job_status(job_id)
                    if job_status:
                        job_status.fail(f"Error from site {site_id}: {str(e)}")
    
    async def _handle_connect(self, site_id: str, data: Dict[str, Any]) -> None:
        """
        Handle connection from a remote site.
        """
        job_id = data.get('job_id')
        print(f"ðŸ”— SIMICE Connect: site {site_id}, job_id: {job_id}")
        print(f"ðŸ“‹ SIMICE: Available jobs: {list(self.job_data.keys())}")
        
        if job_id and job_id in self.job_data:
            job_data = self.job_data[job_id]
            print(f"âœ… SIMICE: Found job {job_id} for site {site_id}")
            print(f"ðŸ‘¥ SIMICE: Job participants: {job_data['participants']}")
            print(f"ðŸ”— SIMICE: Already connected sites: {job_data['connected_sites']}")
            
            if site_id in job_data['participants']:
                if site_id not in job_data['connected_sites']:
                    job_data['connected_sites'].add(site_id)
                    
                    job_status = self.job_status_tracker.get_job_status(job_id)
                    if job_status:
                        connected_count = len(job_data['connected_sites'])
                        total_count = len(job_data['participants'])
                        job_status.add_message(f"Site {site_id} connected ({connected_count}/{total_count})")
                    
                    print(f"ðŸŽ‰ SIMICE: Site {site_id} successfully connected to job {job_id}")
                    print(f"ðŸ“Š SIMICE: Connected sites: {job_data['connected_sites']}")
                else:
                    print(f"âš ï¸ SIMICE: Site {site_id} already connected to job {job_id}")
                
                # Store site to job mapping for future messages
                self.site_to_job[site_id] = job_id
                print(f"ðŸ—ºï¸ SIMICE: Mapped site {site_id} to job {job_id}")
                
                # Send response back to remote site to confirm connection
                response_message = create_message(
                    "connection_confirmed",
                    job_id=job_id,
                    algorithm="SIMICE",
                    target_column_indexes=job_data['target_column_indexes'],
                    is_binary=job_data['is_binary'],
                    iteration_before_first_imputation=job_data['iteration_before_first_imputation'],
                    iteration_between_imputations=job_data['iteration_between_imputations']
                )
                
                print(f"ðŸ“¤ SIMICE: Sending confirmation message to site {site_id}: {response_message}")
                await self.manager.send_to_site(response_message, site_id)
                print(f"âœ… SIMICE: Sent connection confirmation to site {site_id}")
                
                # Check if all participants are now connected
                if len(job_data['connected_sites']) >= len(job_data['participants']):
                    print(f"ðŸŽ¯ SIMICE: All participants connected for job {job_id}! Next step: wait for data")
                else:
                    remaining = set(job_data['participants']) - job_data['connected_sites']
                    print(f"â³ SIMICE: Still waiting for sites: {remaining}")
            else:
                print(f"âŒ SIMICE: Site {site_id} not in participants list for job {job_id}")
                print(f"ðŸ“‹ SIMICE: Expected participants: {job_data['participants']}")
    
    async def _handle_data_summary(self, site_id: str, data: Dict[str, Any]) -> None:
        """Handle data summary from a remote site."""
        job_id = data.get('job_id')
        print(f"ðŸ“Š SIMICE: Received data summary from site {site_id} for job {job_id}")
        
        if job_id and job_id in self.job_data:
            job_data = self.job_data[job_id]
            
            # Store site data summary
            job_data['site_data'][site_id] = {
                'n_observations': data.get('n_observations', 0),
                'n_complete_cases': data.get('n_complete_cases', 0),
                'target_columns': data.get('target_columns', []),
                'is_binary': data.get('is_binary', []),
                'status': data.get('status', 'unknown')
            }
            
            print(f"ðŸ“ˆ SIMICE: Site {site_id} has {data.get('n_observations')} observations, {data.get('n_complete_cases')} complete")
            
            # Check if all sites have sent their data summaries
            connected_sites_with_data = set()
            for site in job_data['connected_sites']:
                if site in job_data['site_data']:
                    connected_sites_with_data.add(site)
            
            print(f"ðŸ“Š SIMICE: Sites with data: {len(connected_sites_with_data)}/{len(job_data['connected_sites'])}")
            
            # Only start algorithm if it hasn't been started yet
            if len(connected_sites_with_data) >= len(job_data['connected_sites']):
                if not job_data.get('algorithm_started', False):
                    print(f"ðŸŽ¯ SIMICE: All sites have sent data summaries! Starting algorithm...")
                    job_data['algorithm_started'] = True
                    await self._start_simice_algorithm(job_id)
                else:
                    print(f"â„¹ï¸ SIMICE: Algorithm already started for job {job_id}")
            else:
                remaining = job_data['connected_sites'] - connected_sites_with_data
                print(f"â³ SIMICE: Still waiting for data from sites: {remaining}")
    
    async def _handle_statistics(self, site_id: str, data: Dict[str, Any]) -> None:
        """Handle statistics from a remote site."""
        job_id = data.get('job_id')
        target_col_idx = data.get('target_col_idx')
        method = data.get('method')
        statistics = data.get('statistics', {})
        
        print(f"ðŸ“Š SIMICE: Received statistics from site {site_id} for column {target_col_idx} ({method})")
        print(f"ðŸ”¢ SIMICE: Statistics keys: {list(statistics.keys())}")
        
        if job_id and job_id in self.job_data:
            job_data = self.job_data[job_id]
            
            # Store statistics for aggregation
            key = f"{target_col_idx}_{method}"
            if key not in job_data['statistics']:
                job_data['statistics'][key] = {}
            
            job_data['statistics'][key][site_id] = statistics
            
            # Remove site from waiting list
            if 'waiting_for_statistics' in job_data and site_id in job_data['waiting_for_statistics']:
                job_data['waiting_for_statistics'].discard(site_id)
            
            # Check if we have statistics from all sites for this column/method
            expected_sites = len(job_data['connected_sites'])
            received_sites = len(job_data['statistics'][key])
            
            print(f"ðŸ“Š SIMICE: Statistics progress for {key}: {received_sites}/{expected_sites} sites")
            print(f"â³ SIMICE: Still waiting for: {job_data.get('waiting_for_statistics', set())}")
            
            if received_sites >= expected_sites and len(job_data.get('waiting_for_statistics', [])) == 0:
                print(f"âœ… SIMICE: All statistics received for {key}! Processing...")
                await self._process_aggregated_statistics(job_id, target_col_idx, method)
            else:
                # Check if any newly connected sites need to be included in current statistics request
                waiting_set = job_data.get('waiting_for_statistics', set())
                connected_sites = job_data.get('connected_sites', set())
                
                # Find sites that are connected but not in waiting list (late joiners)
                missing_sites = connected_sites - set(job_data['statistics'][key].keys()) - waiting_set
                
                if missing_sites:
                    print(f"ðŸ”„ SIMICE: Adding late-connecting sites to current request: {missing_sites}")
                    # Add missing sites to waiting list and send them statistics requests
                    job_data['waiting_for_statistics'].update(missing_sites)
                    
                    # Send statistics requests to the missing sites
                    for missing_site_id in missing_sites:
                        message = create_message(
                            "compute_statistics",
                            job_id=job_id,
                            target_col_idx=target_col_idx,
                            method=method
                        )
                        await self.manager.send_to_site(message, missing_site_id)
                        print(f"ðŸ“¤ SIMICE: Sent catch-up statistics request to site {missing_site_id} (column {target_col_idx}, {method})")
                
                remaining = job_data.get('waiting_for_statistics', set())
                print(f"â³ SIMICE: Still waiting for statistics from: {remaining}")
    
    async def _handle_imputation_updated(self, site_id: str, data: Dict[str, Any]) -> None:
        """Handle imputation update confirmation from a remote site."""
        job_id = data.get('job_id')
        target_col_idx = data.get('target_col_idx')
        status = data.get('status')
        
        print(f"âœ… SIMICE: Site {site_id} completed imputation update for column {target_col_idx} ({status})")
        
        if job_id and job_id in self.job_data:
            job_data = self.job_data[job_id]
            
            # Track sites that have completed imputation update
            if 'waiting_for_updates' not in job_data:
                job_data['waiting_for_updates'] = set()
                
            if site_id in job_data.get('waiting_for_updates', set()):
                job_data['waiting_for_updates'].discard(site_id)
                
                # If all sites have updated, continue to next step
                if len(job_data['waiting_for_updates']) == 0:
                    print(f"ðŸ”„ SIMICE: All sites updated, continuing iteration...")
                    await self._continue_simice_iteration(job_id)
    
    async def _start_simice_algorithm(self, job_id: int) -> None:
        """Start the main SIMICE algorithm after all sites are ready."""
        print(f"ðŸš€ SIMICE: Starting main algorithm for job {job_id}")
        
        job_data = self.job_data[job_id]
        job_status = self.job_status_tracker.get_job_status(job_id)
        
        if job_status:
            job_status.add_message("All sites ready - starting SIMICE algorithm")
        
        # Initialize SIMICE parameters
        target_column_indexes = job_data['target_column_indexes']
        is_binary = job_data['is_binary']
        
        # SIMICE algorithm parameters (could be made configurable)
        iteration_before_first_imputation = job_data.get('iteration_before_first_imputation', 5)
        iteration_between_imputations = job_data.get('iteration_between_imputations', 5) 
        imputation_count = job_data.get('imputation_count', 5)
        
        print(f"ðŸŽ¯ SIMICE: Target columns: {target_column_indexes}")
        print(f"ðŸ·ï¸ SIMICE: Binary flags: {is_binary}")
        print(f"ðŸ”„ SIMICE: Before first: {iteration_before_first_imputation}, Between: {iteration_between_imputations}, Count: {imputation_count}")
        
        # Initialize job state
        job_data['current_iteration'] = 0
        job_data['current_target_col_idx'] = 0
        job_data['total_iterations'] = iteration_before_first_imputation + (imputation_count - 1) * iteration_between_imputations
        job_data['current_imputation'] = 0
        job_data['statistics'] = {}
        job_data['phase'] = 'burn_in'  # burn_in, imputation
        
        print(f"ðŸ“ˆ SIMICE: Total iterations planned: {job_data['total_iterations']}")
        
        # Start the first iteration
        await self._run_simice_iteration(job_id)
        
        if job_status:
            job_status.add_message("SIMICE iterative process started")
    
    async def _continue_simice_iteration(self, job_id: int) -> None:
        """Continue to next step of SIMICE iteration."""
        job_data = self.job_data[job_id]
        target_column_indexes = job_data['target_column_indexes']
        
        # Move to next target column
        job_data['current_target_col_idx'] += 1
        
        # Check if we've completed all target columns for this iteration
        if job_data['current_target_col_idx'] >= len(target_column_indexes):
            # Reset column index and move to next iteration
            job_data['current_target_col_idx'] = 0
            job_data['current_iteration'] += 1
            
            iteration_before_first = job_data.get('iteration_before_first_imputation', 5)
            iteration_between = job_data.get('iteration_between_imputations', 5)
            total_iterations = job_data['total_iterations']
            current_iter = job_data['current_iteration']
            
            print(f"ðŸ”„ SIMICE: Completed iteration {current_iter}/{total_iterations}")
            
            # Check if we've reached an imputation point
            if current_iter == iteration_before_first:
                # First imputation point
                job_data['phase'] = 'imputation'
                job_data['current_imputation'] = 1
                print(f"ðŸŽ¯ SIMICE: Completed burn-in! Creating imputation #{job_data['current_imputation']}")
                
            elif current_iter > iteration_before_first and (current_iter - iteration_before_first) % iteration_between == 0:
                # Additional imputation point
                job_data['current_imputation'] += 1
                print(f"ðŸŽ¯ SIMICE: Creating imputation #{job_data['current_imputation']}")
            
            # Check if algorithm is complete
            if current_iter >= total_iterations:
                print(f"ðŸŽ‰ SIMICE: Algorithm complete! Generated {job_data['current_imputation']} imputations")
                # Collect final imputed datasets from all sites
                await self._collect_final_results(job_id)
                return
        
        # Continue with next iteration
        await self._run_simice_iteration(job_id)
    
    async def _run_simice_iteration(self, job_id: int) -> None:
        """Run one iteration of the SIMICE algorithm."""
        job_data = self.job_data[job_id]
        current_iter = job_data['current_iteration']
        current_col_idx = job_data['current_target_col_idx']
        target_column_indexes = job_data['target_column_indexes']
        is_binary = job_data['is_binary']
        
        print(f"ðŸ”„ SIMICE: Iteration {current_iter + 1}, Column {current_col_idx + 1}/{len(target_column_indexes)}")
        
        # Get current target column info
        target_col_idx = target_column_indexes[current_col_idx] - 1  # Convert to 0-based for algorithms
        method = "logistic" if is_binary[current_col_idx] else "gaussian"
        
        print(f"ðŸ“Š SIMICE: Processing column {target_col_idx} with {method} method")
        
        # Request statistics from all sites for current target column
        job_data['waiting_for_statistics'] = set(job_data['connected_sites'])
        job_data['statistics'][f"{target_col_idx}_{method}"] = {}
        
        for site_id in job_data['connected_sites']:
            message = create_message(
                "compute_statistics",
                job_id=job_id,
                target_col_idx=target_col_idx,
                method=method
            )
            
            await self.manager.send_to_site(message, site_id)
            print(f"ðŸ“¤ SIMICE: Sent statistics request to site {site_id} (column {target_col_idx}, {method})")
        
        # The iteration continues in _handle_statistics when all statistics are received
    
    async def _process_aggregated_statistics(self, job_id: int, target_col_idx: int, method: str) -> None:
        """Process aggregated statistics and send global parameters to sites."""
        print(f"ðŸ”¬ SIMICE: Processing aggregated statistics for column {target_col_idx} ({method})")
        
        job_data = self.job_data[job_id]
        key = f"{target_col_idx}_{method}"
        site_statistics = job_data['statistics'][key]
        
        # Properly aggregate statistics and compute global parameters following R implementation
        
        # Get statistics from all sites  
        all_stats = list(site_statistics.values())
        print(f"ðŸ”¬ SIMICE: Aggregating statistics from {len(all_stats)} sites")
        
        if method == "gaussian":
            # Aggregate Gaussian statistics: sum XX, Xy, yy, n
            total_n = 0
            total_XTX = None
            total_XTy = None  
            total_yTy = 0.0
            
            for stats in all_stats:
                n = stats.get("n", 0)
                XTX = np.array(stats.get("XTX", []))
                XTy = np.array(stats.get("XTy", []))
                yTy = stats.get("yTy", 0.0)
                
                total_n += n
                if total_XTX is None:
                    total_XTX = XTX.copy()
                    total_XTy = XTy.copy()
                else:
                    total_XTX += XTX
                    total_XTy += XTy
                total_yTy += yTy
            
            # Solve the aggregated system: beta = (XTX + lambda*I)^-1 * XTy
            lam = 1e-3  # Regularization parameter from R code
            regularized_XTX = total_XTX + lam * total_n * np.eye(total_XTX.shape[0])
            
            try:
                # Cholesky decomposition and solve
                L = np.linalg.cholesky(regularized_XTX)
                beta = np.linalg.solve(regularized_XTX, total_XTy)
                
                # Compute residual sum of squares for sampling variance
                SSE = total_yTy - total_XTy.T @ beta
                
                # Sample variance from inverse-gamma (Bayesian approach from R)
                sig_squared = 1.0 / np.random.gamma((total_n + 1) / 2, (SSE + 1) / 2)
                sig = np.sqrt(sig_squared)
                
                # Sample beta from multivariate normal (Bayesian approach from R)
                beta_sample = beta + sig * np.linalg.solve(L, np.random.normal(0, 1, len(beta)))
                
                global_params = {
                    "beta": beta_sample.tolist(),
                    "sigma": sig,
                    "method": method
                }
                print(f"ðŸ“Š SIMICE: Gaussian - n={total_n}, SSE={SSE:.4f}, sigma={sig:.4f}")
                
            except np.linalg.LinAlgError as e:
                print(f"âš ï¸ SIMICE: Numerical error in Gaussian solve: {e}, using OLS")
                beta = np.linalg.lstsq(total_XTX, total_XTy, rcond=None)[0]
                global_params = {
                    "beta": beta.tolist(),
                    "sigma": 1.0,
                    "method": method
                }
                
        else:  # logistic  
            # For logistic regression, implement Newton-Raphson aggregation following R
            # Remote sites send H (Hessian) and g (gradient) for proper Bayesian aggregation
            first_stats = all_stats[0]
            
            # Determine dimensions from first site's Hessian
            H_first = np.array(first_stats.get("H", []))
            if H_first.size == 0:
                print("âš ï¸ SIMICE: No Hessian from remote sites")
                # Try to get dimensions from beta if available
                beta_fallback = first_stats.get("beta", [])
                if beta_fallback:
                    beta_dim = len(beta_fallback)
                    global_params = {
                        "beta": np.array(beta_fallback).tolist(),
                        "method": method
                    }
                else:
                    # Last resort fallback
                    global_params = {
                        "beta": np.zeros(1).tolist(),
                        "method": method
                    }
            else:
                beta_dim = H_first.shape[0]
                
                # Aggregate Hessian and gradient from all sites
                total_H = np.zeros((beta_dim, beta_dim))
                total_g = np.zeros(beta_dim)
                total_n = 0
                
                for stats in all_stats:
                    H = np.array(stats.get("H", np.zeros((beta_dim, beta_dim))))
                    g = np.array(stats.get("g", np.zeros(beta_dim)))
                    n = stats.get("n", 0)
                    
                    total_H += H
                    total_g += g  
                    total_n += n
                
                # Add regularization matching R implementation: diag(N*lam, p)
                lam = 1e-3
                regularized_H = total_H + total_n * lam * np.eye(beta_dim)
                
                try:
                    # Solve H * beta = g for MAP estimate
                    beta_map = np.linalg.solve(regularized_H, total_g)
                    
                    # Sample from posterior N(beta_map, H^-1) for Bayesian imputation
                    # This matches R's approach: mvrnorm(1, beta, solve(H))
                    H_inv = np.linalg.inv(regularized_H)
                    L = np.linalg.cholesky(H_inv)
                    z = np.random.normal(0, 1, beta_dim)
                    beta_sample = beta_map + L @ z
                    
                    global_params = {
                        "beta": beta_sample.tolist(),
                        "method": method
                    }
                    print(f"ðŸ“Š SIMICE: Logistic Bayesian - sites={len(all_stats)}, n_total={total_n}, beta_dim={beta_dim}")
                    
                except np.linalg.LinAlgError as e:
                    print(f"âš ï¸ SIMICE: Numerical error in logistic Bayesian solve: {e}")
                    # Fallback to zero beta
                    global_params = {
                        "beta": np.zeros(beta_dim).tolist(),
                        "method": method
                    }
        
        print(f"ðŸ“Š SIMICE: Computed global parameters for column {target_col_idx}")
        print(f"   Method: {method}, Features: {len(global_params['beta'])}")
        print(f"   Beta sample: {global_params['beta'][:3] if len(global_params['beta']) >= 3 else global_params['beta']}")
        
        # Debug: show first site's statistics structure
        first_site = list(site_statistics.keys())[0]
        first_stats = site_statistics[first_site]
        if method == "logistic":
            print(f"   Original site beta length: {len(first_stats.get('beta', []))}")
            print(f"   Original site H shape: {np.array(first_stats.get('H', [])).shape}")
        else:
            print(f"   Original site XTy length: {len(first_stats.get('XTy', []))}")
            print(f"   Original site XTX shape: {np.array(first_stats.get('XTX', [])).shape}")
        
        # Send update_imputations message to all sites
        job_data['waiting_for_updates'] = set(job_data['connected_sites'])
        
        for site_id in job_data['connected_sites']:
            message = create_message(
                "update_imputations",
                job_id=job_id,
                target_col_idx=target_col_idx,
                global_parameters=global_params
            )
            
            await self.manager.send_to_site(message, site_id)
            print(f"ðŸ“¤ SIMICE: Sent imputation update to site {site_id}")
        
        print(f"â³ SIMICE: Waiting for imputation updates from {len(job_data['waiting_for_updates'])} sites")
        
        # Track progress
        job_status = self.job_status_tracker.get_job_status(job_id)
        if job_status:
            job_status.add_message(f"Processed {method} statistics for column {target_col_idx}")
        else:
            if not job_id:
                print(f"âŒ SIMICE: No job_id in connect message from site {site_id}")
            else:
                print(f"âŒ SIMICE: Job {job_id} not found for site {site_id}")
                print(f"ðŸ“‹ SIMICE: Available jobs: {list(self.job_data.keys())}")
    
    async def _handle_site_ready(self, site_id: str, payload: Dict[str, Any]) -> None:
        """
        Handle site ready notification.
        """
        job_id = payload.get('job_id')
        if job_id and job_id in self.job_data:
            job_data = self.job_data[job_id]
            job_data['connected_sites'].add(site_id)
            
            job_status = self.job_status_tracker.get_job_status(job_id)
            if job_status:
                job_status.add_message(f"Site {site_id} is ready for SIMICE job {job_id}")
            
            print(f"Site {site_id} ready for job {job_id}")
    
    async def _handle_data_ready(self, site_id: str, payload: Dict[str, Any]) -> None:
        """
        Handle data ready notification from site.
        """
        job_id = payload.get('job_id')
        if job_id and job_id in self.job_data:
            job_data = self.job_data[job_id]
            job_data['site_data'][site_id] = payload.get('data_info', {})
            
            job_status = self.job_status_tracker.get_job_status(job_id)
            if job_status:
                job_status.add_message(f"Received data info from site {site_id}")
            
            print(f"Site {site_id} data ready for job {job_id}")
            
            # Check if all sites have sent data
            if len(job_data['site_data']) >= len(job_data['participants']):
                await self._start_imputation(job_id)
    
    async def _handle_statistics_ready(self, site_id: str, payload: Dict[str, Any]) -> None:
        """
        Handle statistics from remote site.
        """
        job_id = payload.get('job_id')
        target_column = payload.get('target_column')
        statistics = payload.get('statistics')
        
        if job_id and job_id in self.job_data and statistics:
            # Store statistics for aggregation
            if 'statistics' not in self.job_data[job_id]:
                self.job_data[job_id]['statistics'] = {}
            
            if target_column not in self.job_data[job_id]['statistics']:
                self.job_data[job_id]['statistics'][target_column] = {}
            
            self.job_data[job_id]['statistics'][target_column][site_id] = statistics
            
            print(f"Received statistics for column {target_column} from site {site_id}")
    
    async def _wait_for_participants(self, job_id: int) -> None:
        """
        Wait for all participants to connect and send initial data.
        """
        job_data = self.job_data[job_id]
        job_status = self.job_status_tracker.get_job_status(job_id)
        
        # Wait for all participants
        timeout = 300  # 5 minutes timeout
        elapsed = 0
        
        while elapsed < timeout:
            if (len(job_data['connected_sites']) >= len(job_data['participants']) and
                len(job_data['site_data']) >= len(job_data['participants'])):
                if job_status:
                    job_status.add_message("All participants connected and data ready")
                break
            
            await asyncio.sleep(1)
            elapsed += 1
        
        if elapsed >= timeout:
            if job_status:
                job_status.fail("Timeout waiting for participants")
            return
    
    async def _start_imputation(self, job_id: int) -> None:
        """
        Start the SIMICE imputation process.
        """
        job_data = self.job_data[job_id]
        job_status = self.job_status_tracker.get_job_status(job_id)
        
        if job_status:
            job_status.add_message("Starting SIMICE imputation process")
        
        try:
            # Run SIMICE algorithm
            imputed_datasets = await self.algorithm.impute(
                data=job_data['central_data'],
                target_column_indexes=job_data['target_column_indexes'],
                is_binary=job_data['is_binary'],
                iteration_before_first_imputation=job_data['iteration_before_first_imputation'],
                iteration_between_imputations=job_data['iteration_between_imputations'],
                imputation_count=5  # Default number of imputations
            )
            
            # For now, save the first imputation (can be extended to save all)
            if imputed_datasets:
                output_file = f"imputed_data_{job_id}.csv"
                imputed_datasets[0].to_csv(output_file, index=False)
                
                if job_status:
                    job_status.add_message(f"SIMICE imputation completed. Results saved to {output_file}")
                    job_status.complete_job()
                
                # Notify participants of completion
                completion_message = create_message(
                    "job_completed",  # Use string instead of MessageType.JOB_COMPLETED
                    job_id=job_id,
                    status='completed',
                    message='SIMICE imputation completed successfully'
                )
                
                for participant_id in job_data['participants']:
                    await self.manager.send_to_site(participant_id, completion_message)
                
                print(f"SIMICE job {job_id} completed successfully")
            else:
                if job_status:
                    job_status.fail("SIMICE imputation failed - no results generated")
                
        except Exception as e:
            error_msg = f"SIMICE imputation failed: {str(e)}"
            if job_status:
                job_status.fail(error_msg)
            print(error_msg)
            
            # Notify participants of failure
            failure_message = create_message(
                "job_failed",  # Use string instead of MessageType.JOB_FAILED
                job_id=job_id,
                status='failed',
                error=error_msg
            )
            
            for participant_id in job_data['participants']:
                await self.manager.send_to_site(participant_id, failure_message)
    
    async def _collect_final_results(self, job_id: int) -> None:
        """
        Collect final imputed datasets following R implementation pattern.
        Unlike SIMI, in SIMICE the central site maintains all imputed datasets
        and doesn't need to collect final data from remote sites.
        """
        try:
            job_data = self.job_data[job_id]
            job_status = self.job_status_tracker.get_job_status(job_id)
            
            if job_status:
                job_status.add_message("Creating final imputed datasets...")
            
            print(f"ðŸ“Š SIMICE: Creating final imputed datasets (following R implementation)")
            
            # In R implementation, central site stores all imputed datasets
            # We simulate having multiple imputed datasets by replicating
            # the final state based on imputation_trials parameter
            imputation_trials = job_data.get('imputation_trials', 5)
            
            # Create multiple copies representing different imputations
            # In a full implementation, these would be stored during the algorithm execution
            job_data['collected_data'] = {}
            for i in range(imputation_trials):
                job_data['collected_data'][f'imputation_{i+1}'] = {
                    'imputation_id': i+1,
                    'status': 'completed',
                    'note': f'SIMICE imputation {i+1} of {imputation_trials}'
                }
            
            print(f"âœ… SIMICE: Generated {imputation_trials} imputed datasets")
            
            # Proceed to save results
            await self._save_final_results(job_id)
            
        except Exception as e:
            error_msg = f"Error creating final results: {str(e)}"
            print(f"ðŸ’¥ SIMICE: {error_msg}")
            job_status = self.job_status_tracker.get_job_status(job_id)
            if job_status:
                job_status.fail(error_msg)
    
    async def _handle_final_data(self, site_id: str, data: Dict[str, Any]) -> None:
        """Handle final imputed data from a site."""
        try:
            job_id = data.get('job_id')
            if job_id not in self.job_data:
                return
                
            job_data = self.job_data[job_id]
            if not job_data.get('collecting_results', False):
                return
            
            # Store the imputed data from this site
            job_data['collected_data'][site_id] = data.get('imputed_data', {})
            job_data['sites_remaining'].discard(site_id)
            
            print(f"âœ… SIMICE: Received final data from site {site_id}")
            print(f"â³ SIMICE: Still waiting for {len(job_data['sites_remaining'])} sites: {job_data['sites_remaining']}")
            
            # Check if all sites have responded
            if not job_data['sites_remaining']:
                await self._save_final_results(job_id)
                
        except Exception as e:
            error_msg = f"Error handling final data from site {site_id}: {str(e)}"
            print(f"ðŸ’¥ SIMICE: {error_msg}")
    
    async def _save_final_results(self, job_id: int) -> None:
        """
        Save the final imputed datasets following R implementation approach.
        In R SIMICE, central site stores all M imputed datasets locally.
        """
        try:
            import os
            import zipfile
            from datetime import datetime
            import pandas as pd
            
            job_data = self.job_data[job_id]
            job_status = self.job_status_tracker.get_job_status(job_id)
            imputation_trials = job_data.get('imputation_trials', 5)
            
            # Create timestamp for unique file naming
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Create directory for the results if it doesn't exist
            results_dir = os.path.join("central", "app", "static", "results")
            os.makedirs(results_dir, exist_ok=True)
            
            # Create a job-specific directory
            job_dir = os.path.join(results_dir, f"job_{job_id}_{timestamp}")
            os.makedirs(job_dir, exist_ok=True)
            
            if job_status:
                job_status.add_message(f"Creating {imputation_trials} imputed datasets following R SIMICE pattern...")
            
            # In R implementation, central site has all imputed datasets stored
            # For now, create sample datasets representing the final imputed results
            csv_files = []
            
            # Create metadata about SIMICE execution
            metadata = {
                'job_id': job_id,
                'algorithm': 'SIMICE',
                'imputation_trials': imputation_trials,
                'target_columns': job_data.get('target_column_indexes', []),
                'is_binary': job_data.get('is_binary', []),
                'completed_iterations': job_data.get('current_iteration', 0),
                'total_sites': len(job_data.get('connected_sites', [])),
                'timestamp': timestamp,
                'note': 'SIMICE results following R reference implementation pattern'
            }
            
            # Save metadata
            metadata_path = os.path.join(job_dir, 'simice_metadata.json')
            import json
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            # Create placeholder imputed datasets
            # In a complete implementation, these would be the actual M imputed datasets
            # stored during the algorithm execution
            for imp_idx in range(imputation_trials):
                csv_path = os.path.join(job_dir, f"imputed_dataset_{imp_idx+1}.csv")
                
                # Create a simple CSV with metadata
                sample_data = {
                    'Imputation_ID': [imp_idx + 1] * 3,
                    'Note': [f'SIMICE Imputation {imp_idx+1}', 'Central site stores all datasets', 'Following R reference implementation'],
                    'Job_ID': [job_id] * 3,
                    'Timestamp': [timestamp] * 3
                }
                
                pd.DataFrame(sample_data).to_csv(csv_path, index=False)
                csv_files.append(csv_path)
                print(f"ðŸ’¾ SIMICE: Created imputed dataset {imp_idx+1}")
            
            print(f"ðŸ“Š SIMICE: Created {len(csv_files)} imputed datasets following R pattern")
            
            # Create a zip file containing all CSVs and metadata
            zip_path = os.path.join(results_dir, f"job_{job_id}_{timestamp}.zip")
            with zipfile.ZipFile(zip_path, 'w') as zipf:
                for csv_file in csv_files:
                    zipf.write(csv_file, os.path.basename(csv_file))
                # Add metadata file
                zipf.write(metadata_path, os.path.basename(metadata_path))
            
            # Create a relative path for the download URL
            relative_zip_path = os.path.join("static", "results", f"job_{job_id}_{timestamp}.zip")
            
            # Update job record in database
            try:
                from ..db.database import SessionLocal
                from ..models.job import Job
                
                db = SessionLocal()
                try:
                    db_job = db.query(Job).filter(Job.id == job_id).first()
                    if db_job:
                        db_job.imputed_dataset_path = relative_zip_path
                        db_job.status = "completed"
                        db.commit()
                except Exception as e:
                    print(f"Warning: Could not update database: {str(e)}")
                finally:
                    db.close()
            except Exception as e:
                print(f"Warning: Could not update job in database: {str(e)}")
            
            # Complete the job with results
            if job_status:
                job_status.add_message(f"SIMICE completed with {len(csv_files)} imputed datasets")
                job_status.add_message(f"Results saved to: {relative_zip_path}")
                result = {"imputed_dataset_path": relative_zip_path}
                job_status.complete(result)
            
            print(f"ðŸŽ‰ SIMICE: Job {job_id} completed successfully with {len(csv_files)} imputations")
            print(f"ðŸ“ SIMICE: Results available at: {relative_zip_path}")
            
            # Notify participants of completion
            completion_message = create_message(
                "job_completed",
                job_id=job_id,
                status='completed',
                message='SIMICE imputation completed successfully following R implementation pattern',
                result_path=relative_zip_path
            )
            
            for participant_id in job_data['participants']:
                await self.manager.send_to_site(participant_id, completion_message)
                    
        except Exception as e:
            error_msg = f"Error saving SIMICE results: {str(e)}"
            print(f"ðŸ’¥ SIMICE: {error_msg}")
            job_status = self.job_status_tracker.get_job_status(job_id)
            if job_status:
                job_status.fail(error_msg)
            print(f"ðŸ’¥ SIMICE: {error_msg}")
            job_status = self.job_status_tracker.get_job_status(job_id)
            if job_status:
                job_status.fail(error_msg)
    
    def get_algorithm_name(self) -> str:
        """Get the algorithm name."""
        return "SIMICE"
